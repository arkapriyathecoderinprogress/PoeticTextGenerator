{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNbY7dlp8CQvnS7i4/1MgCD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arkapriyathecoderinprogress/PoeticTextGenerator/blob/main/PoeticTextGenerator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "BGCn20f_ELp_"
      },
      "outputs": [],
      "source": [
        "#Import necessary libraries\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Loading Shakespeare Texts\n",
        "#Download the Shakespeare dataset\n",
        "path_to_file=tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n",
        "\n",
        "#Read the data\n",
        "text=open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "print(f\"Length of text: {len(text)} characters\")\n",
        "\n",
        "#Take a look at the first 250 characters in the text\n",
        "print(text[:250])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "It_B8XxG5JAH",
        "outputId": "8a74eea9-a186-444b-e41f-c13f82fd601b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "1115394/1115394 [==============================] - 0s 0us/step\n",
            "Length of text: 1115394 characters\n",
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Preparing data\n",
        "#Create a set of unique characters in the text\n",
        "vocab=sorted(set(text))\n",
        "print(f\"{len(vocab)} unique characters\")\n",
        "\n",
        "#Create a mapping from characters to indices and vice versa\n",
        "char2idx={u:i for i,u in enumerate(vocab)}\n",
        "idx2char=np.array(vocab)\n",
        "\n",
        "#Convert the text to integer representation\n",
        "text_as_int=np.array([char2idx[c] for c in text])\n",
        "\n",
        "#Define the maximum length of the sequences\n",
        "seq_length=100\n",
        "examples_per_epoch=len(text)//seq_length\n",
        "\n",
        "#Create training examples/targets\n",
        "char_dataset=tf.data.Dataset.from_tensor_slices(text_as_int)\n",
        "sequences=char_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "def split_input_target(chunk):\n",
        "  input_text=chunk[:-1]\n",
        "  target_text=chunk[:1]\n",
        "  return input_text,target_text\n",
        "\n",
        "dataset=sequences.map(split_input_target)\n",
        "\n",
        "#Batch size and buffer size\n",
        "BATCH_SIZE=64\n",
        "BUFFER_SIZE=10000\n",
        "\n",
        "dataset=dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bElSXeuV7WSu",
        "outputId": "398be4bb-0b5f-4491-958a-7091761501c7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "65 unique characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#3. Building Recurrent Neural Network\n",
        "#Length of vocabulary in chars\n",
        "vocab_size=len(vocab)\n",
        "\n",
        "#The embedding dimension\n",
        "embedding_dim=256\n",
        "\n",
        "#Number of RNN units\n",
        "rnn_units=1024\n",
        "\n",
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "  model=tf.keras.Sequential([\n",
        "      tf.keras.layers.Embedding(vocab_size, embedding_dim, batch_input_shape=[batch_size,None]),\n",
        "      tf.keras.layers.GRU(rnn_units,\n",
        "                          return_sequences=True,\n",
        "                          stateful=True,\n",
        "                          recurrent_initializer='glorot_uniform'),\n",
        "      tf.keras.layers.Dense(vocab_size)\n",
        "  ])\n",
        "  return model\n",
        "\n",
        "model=build_model(\n",
        "    vocab_size=len(vocab),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units,\n",
        "    batch_size=BATCH_SIZE\n",
        ")"
      ],
      "metadata": {
        "id": "S0KeXWQM-V9I"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Debug: Print dataset shape\n",
        "for input_example, target_example in dataset.take(1):\n",
        "    print(f\"Input shape: {input_example.shape}\")\n",
        "    print(f\"Target shape: {target_example.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cQx-MIRXHo9a",
        "outputId": "b097e8e5-13ff-49c7-eecd-47f65926ff71"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape: (64, 100)\n",
            "Target shape: (64, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Helper Function\n",
        "#Function to compute the loss\n",
        "def loss(labels,logits):\n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "\n",
        "model.compile(optimizer='adam', loss=loss)\n",
        "\n",
        "#Directory where the checkpoints will be saved\n",
        "checkpoint_dir='./training_checkpoints'\n",
        "checkpoint_prefix=os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True\n",
        ")"
      ],
      "metadata": {
        "id": "-TOa2iHGAMCE"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Generating Text\n",
        "#Define the number of epochs\n",
        "EPOCHS = 20\n",
        "\n",
        "#Train the model\n",
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])\n",
        "\n",
        "#Function to generate text\n",
        "def generate_text(model, start_string, temperature=1.0):\n",
        "    # Number of characters to generate\n",
        "    num_generate = 1000\n",
        "\n",
        "    #Converting our start string to numbers (vectorizing)\n",
        "    input_eval = [char2idx[s] for s in start_string]\n",
        "    input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "    #Empty string to store our results\n",
        "    text_generated = []\n",
        "\n",
        "    #Here batch size == 1\n",
        "    model.reset_states()\n",
        "    for i in range(num_generate):\n",
        "        predictions = model(input_eval)\n",
        "        predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "        #Using a categorical distribution to predict the character returned by the model\n",
        "        predictions = predictions / temperature\n",
        "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1, 0].numpy()\n",
        "\n",
        "        #The predicted character is passed as the next input to the model\n",
        "        input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "        text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "    return start_string + ''.join(text_generated)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rm4Sh1QEDQDO",
        "outputId": "c04037ef-a748-4e91-ef4c-b731f2742c67"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "172/172 [==============================] - 965s 6s/step - loss: 2.6317\n",
            "Epoch 2/20\n",
            "172/172 [==============================] - 952s 6s/step - loss: 1.9515\n",
            "Epoch 3/20\n",
            "172/172 [==============================] - 948s 5s/step - loss: 1.6856\n",
            "Epoch 4/20\n",
            "172/172 [==============================] - 958s 6s/step - loss: 1.5394\n",
            "Epoch 5/20\n",
            "172/172 [==============================] - 961s 6s/step - loss: 1.4525\n",
            "Epoch 6/20\n",
            "172/172 [==============================] - 953s 6s/step - loss: 1.3926\n",
            "Epoch 7/20\n",
            "172/172 [==============================] - 955s 6s/step - loss: 1.3474\n",
            "Epoch 8/20\n",
            "172/172 [==============================] - 948s 6s/step - loss: 1.3094\n",
            "Epoch 9/20\n",
            "172/172 [==============================] - 959s 6s/step - loss: 1.2745\n",
            "Epoch 10/20\n",
            "172/172 [==============================] - 944s 5s/step - loss: 1.2418\n",
            "Epoch 11/20\n",
            "172/172 [==============================] - 1026s 6s/step - loss: 1.2086\n",
            "Epoch 12/20\n",
            "172/172 [==============================] - 958s 6s/step - loss: 1.1765\n",
            "Epoch 13/20\n",
            "172/172 [==============================] - 947s 5s/step - loss: 1.1427\n",
            "Epoch 14/20\n",
            "172/172 [==============================] - 940s 5s/step - loss: 1.1093\n",
            "Epoch 15/20\n",
            "172/172 [==============================] - 950s 6s/step - loss: 1.0738\n",
            "Epoch 16/20\n",
            "172/172 [==============================] - 950s 6s/step - loss: 1.0387\n",
            "Epoch 17/20\n",
            "172/172 [==============================] - 947s 5s/step - loss: 1.0025\n",
            "Epoch 18/20\n",
            "172/172 [==============================] - 994s 6s/step - loss: 0.9653\n",
            "Epoch 19/20\n",
            "172/172 [==============================] - 1003s 6s/step - loss: 0.9309\n",
            "Epoch 20/20\n",
            "172/172 [==============================] - 959s 6s/step - loss: 0.8965\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Results\n",
        "#Load the dataset checkpoint\n",
        "model=build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "model.build(tf.TensorShape([1, None]))\n",
        "\n",
        "#Generate text with low temperature\n",
        "print(\"Generated text with low temperature (0.5):\\n\")\n",
        "print(generate_text(model, start_string=\"ROMEO: \", temperature=0.5))\n",
        "\n",
        "#Generate text with high temperature\n",
        "print(\"\\nGenerated text with high temperature (1.5):\\n\")\n",
        "print(generate_text(model, start_string=\"ROMEO: \", temperature=1.5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UmhCWdAEVm9H",
        "outputId": "15aaed1a-6175-41e3-ecd0-69fcc75a2c06"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated text with low temperature (0.5):\n",
            "\n",
            "ROMEO: there is my power,\n",
            "And let them know the sea that is no wife.\n",
            "\n",
            "PAULINA:\n",
            "Good morrow, Catesby,\n",
            "What corns it then? what can you stial to the sea?\n",
            "\n",
            "SICINIUS:\n",
            "Peace!\n",
            "The devils are out of door!\n",
            "\n",
            "ANGELO:\n",
            "That he is to be the thing that will be done!\n",
            "Thou and thy conscience sake, to see him any here?\n",
            "\n",
            "Provost:\n",
            "This is a gentleman to be a prisoner to the king;\n",
            "For then the sea with hasty Kate,\n",
            "And then the seat before him, where the death\n",
            "Was wise and virtuous and woman's leave.\n",
            "\n",
            "Second Servant:\n",
            "My lord, I cannot speak a word.\n",
            "\n",
            "ARCHIDAMUS:\n",
            "Why, that will find the king at Oxford.\n",
            "\n",
            "HENRY BOLINGBROKE:\n",
            "Many and heir and to stand upon my company.\n",
            "\n",
            "Both:\n",
            "To bring you so to me, in the duke's death?\n",
            "O, that you look'd for being the deed to save thee,\n",
            "I should not be distributed.\n",
            "\n",
            "Provost:\n",
            "Peace that hath brought you for a poison, nor witness to\n",
            "the time, when that is not a drum dead mouth\n",
            "To play the time to come. This man's bosom first when he did know\n",
            "What answer'd. I would furl our pitious tears \n",
            "\n",
            "Generated text with high temperature (1.5):\n",
            "\n",
            "ROMEO: not years?\n",
            "Which-fondle that I'll together got\n",
            "Men ne's a whitefusely won his country.\n",
            "\n",
            "Third Servingman:\n",
            "Are, gett's towner? My ach-sellinging for security; yet killon congealt in\n",
            "'beciushs; till then were here, or any coxs\n",
            "Tell him your blots gn and nd,\n",
            "Tuil trempable to thanks,\n",
            "After their infaste world's weasons.\n",
            "\n",
            "ANGELO:\n",
            "\n",
            "GONZALO:\n",
            "Thenk we hath sawd them created. There is noked wombtorfeit yield at Warwick's face is dead.\n",
            "\n",
            "LEONTES:\n",
            "Oney,\n",
            "Yor did commend to usinap the plack:\n",
            "Let him but within these enemies\n",
            "double acquainten Hanfords are! Nutscury, what, say.\n",
            "First, mine, sun too? shall Forthy Romeo,\n",
            "Who withoutlservy disdainmflow to\n",
            "Att. My feast, wabrs, by him their lies\n",
            "Thy watchinfure bunish'd, I hate\n",
            "And city; therewar's wife, Clbonn and full of hell.\n",
            "J's dagger op'd pleass to help\n",
            "Sirther urgedif like faith. Bestrigonde haste EFwoman?\n",
            "what doseth unrevenge! Bloody Clarence:\n",
            "Near help my logd, laying, he apprayed himing witely:\n",
            "Which, by that you might provese you to attempt;\n",
            "\n"
          ]
        }
      ]
    }
  ]
}